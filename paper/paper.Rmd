---
title: "Final Paper"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(ggplot2)
library(dplyr)
library(tidyr)
library(magrittr)
library(ggmap)
library(tidytext)
library(maps)
library(extrafont)
library(wordcloud)
library(stringr)
library(purrr)
source("../functions/WordCloud.R")
source("../functions/PercentAccuracy.R")
source("../functions/MeanStar.R")
font_import()
loadfonts()
```

## Introduction

Whenever a person hears about a business or think about starting their own, they are always told that a business’s success is all about location, location, location. It’s hard to quantify how much of a business’s success is determined by its location, but we were curious to see if location was linked to higher star ratings on Yelp. Yelp is a service that publishes crowd-sourced reviews about local business online. Additionally, we wanted to see if there was any relationship between certain attributes of restaurants and their ratings on Yelp.

Aside from analyzing the success and behavior of businesses, we also decided to explore the relationship between the text in reviews and the number of stars it received. In our project, we used a deep dataset of Round 7 of the Yelp Academic Dataset challenge, available at: https://www.yelp.com/dataset_challenge/dataset. The dataset includes five json files of business, check-in, user, review, and tip data, but our research only involved looking at the business, user, and review datasets.

## Exploring the Data
###1. Data Properties
The dataset includes 2.2 million Yelp reviews for 77,455 businesses across the United States. To gain access to the dataset, a name and email had to be provided and the terms of use had to be accepted. The schematic for the business, user, and review datasets were: 

{
    'type': 'business',
    'business_id': (encrypted business id),
    'name': (business name),
    'neighborhoods': [(hood names)],
    'full_address': (localized address),
    'city': (city),
    'state': (state),
    'latitude': latitude,
    'longitude': longitude,
    'stars': (star rating, rounded to half-stars),
    'review_count': review count,
    'categories': [(localized category names)]
    'open': True / False (corresponds to closed, not business hours),
    'hours': {
        (day_of_week): {
            'open': (HH:MM),
            'close': (HH:MM)
        },
        ...
    },
    'attributes': {
        (attribute_name): (attribute_value),
        ...
    },
}

{
    'type': 'review',
    'business_id': (encrypted business id),
    'user_id': (encrypted user id),
    'stars': (star rating, rounded to half-stars),
    'text': (review text),
    'date': (date, formatted like '2012-03-14'),
    'votes': {(vote type): (count)},
}

{
    'type': 'user',
    'user_id': (encrypted user id),
    'name': (first name),
    'review_count': (review count),
    'average_stars': (floating point average, like 4.31),
    'votes': {(vote type): (count)},
    'friends': [(friend user_ids)],
    'elite': [(years_elite)],
    'yelping_since': (date, formatted like '2012-03'),
    'compliments': {
        (compliment_type): (num_compliments_of_this_type),
        ...
    },
    'fans': (num_fans),
}

 
###2. Data Pre-Processing
Data pre-processing were the steps taken to collect and prepare the input data for data mining and visualization. 

####Stage One: JSON to CSV
Converting the raw data from JSON was quite easy using R libraries. We began by reading in the data and writing the tidied data-frame to CSV objects to be read in our analyses. Once this was completed, other R libraries were used to wrangle and tidy the data for their final transformations.

####Stage Two: Data Wrangling
We began by filtering for businesses with more than 300 reviews in the United States. Looking more closely at the business dataset, we noticed a nested data-frame of attributes for each business, so we cleaned and gathered the data so that each row contained one business attribute. Business data set was used to see what makes a business popular and successful; especially by looking at where most-reviewed are clustered and the influence of major business attributes.

Review data was cleaned for sentiment analysis using a library called “tidytext”. After reading in the review dataset, we randomly sampled 500,000 observations. With this, we tokenized each word in a review into a single row in the data-frame and removed stop words like “a”, “and”, “the”, etc. since they had no indication of the reviewers’ attitudes. Using built-in lexicons, we assigned each word a valence score and computed the mean score for each review. Additionally, we created per-word summaries, computing the number of business and reviews the word appeared in and the average star rating based on each review. We filtered for words that appeared in more than 10 businesses and 500 reviews to exclude rare words with strange valence scores and ratings. Using these cleaned datasets, we were able to create visualizations and make inferences discussed later.

```{r load, echo = FALSE}
reviews <- read_csv("../clean_data/yelp_reviews.csv")
reviews <- reviews %>% 
  sample_n(500000)
```

Lastly, business data, user data and review data were all used to analyze the users’ behaviors when they are in unfamiliar places. User data was cleaned to filter for active users with a significant amount of reviews. Cleaned review data and business data were then joined together with user data. We created an additional boolean column called ‘most_reviewed’, after computing the most familiar location for a user (one state per user where most of his/her reviews reside). Using this, we cleaned up and created a new data frame with average star ratings of familiar places and unfamiliar places per user. In the next section, we discuss this further along with results. 

####Stage Three: Pre-Processing using ‘tidytext’ for Sentiment Analysis
Sentiment analysis, or opinion mining, makes use of natural language processing to analyze pieces of text and determine the attitude of the speaker or writer. To perform sentiment analysis, we researched various text mining libraries. First, _tm_ was used to remove stop words, common English words that would give no information, but the process of splitting text reviews into one word-per-row and removing stop words was too inefficient. After more research, we came across “tidytext”, a library for text mining. Using this library, cleaning and aggregating the review data was extremely easy and efficient. We then performed tidy analysis to strip reviews and tokenize each word into a single row in the data-frame.

To explore this data, we researched different lexicons and found ANEW (Affective Norms for English Words), a "sentiment lexicon" that scores words for valence. However, upon further research, we came across an article written by Finn Arup Nielsen, where he compares a sentiment lexicon he created to ANEW. His lexicon, AFINN, contains a list of English words that are assigned an integer value from -5 (negative) to +5 (positive) based on the word’s sentiment. He concluded that his word list better classifies sentiment rating than ANEW on Twitter sentiment analysis. Since Yelp text reviews are similar to tweets, we used AFINN for this sentiment analysis. 

##Results and Discussion
####B. Sentiment Analysis
We hypothesized that words with higher valence scores would produce higher Yelp star ratings for businesses. First, using the R library “wordcloud”, we formed two word clouds for the 100 most frequent words in text reviews with one-star and five-star ratings to determine if there was a distinct difference between the words used.

```{r wordcloud, echo = FALSE, message = FALSE}
reviews_text <- read_csv("../clean_data/reviews_text.csv")
one <- reviews_text %>% 
  filter(stars == 1) %>% 
  group_by(word) %>% 
  count() %>% 
  top_n(100) %>% 
  arrange(n)

five <- reviews_text %>% 
  filter(stars == 5) %>% 
  group_by(word) %>% 
  count() %>% 
  top_n(100) %>% 
  arrange(n)

red <- c("salmon1", "tomato2", "orangered3", "red3", "firebrick4")
WordCloud(one$word, one$n, red)
green <- c("chartreuse2", "mediumseagreen", "forestgreen", 
           "seagreen", "darkgreen")
WordCloud(five$word, five$n, green)
```

While words like “food” and “service” were the most common, there is a noticeable difference between the two word clouds generated. To further explore this difference, we generated boxplots of mean sentiment score versus business star rating to crudely determine if there was a positive trend for valence score and business rating.

```{r boxplots, echo = FALSE, message = FALSE, fig.align = "center"}
review_scores <- read_csv("../clean_data/review_scores.csv")
ggplot(review_scores) +
  geom_boxplot(aes(stars, sentiment, group = stars)) +
  labs(x = "Business Star Rating",
       y = "Mean Sentiment Score",
       title = "Mean Sentiment Score vs. Business Star Rating") +
  theme(text = element_text(family = "Times New Roman"))
```

There was indeed a positive correlation, despite some words with high valence scores appearing in reviews with low ratings and vice versa. Now to determine which words were indicative of a positive review and a negative review and the accuracy of our valence score classifier, using the word summaries data we generated a scatterplot labeled by word, colored by AFINN score, and plotted by average star rating versus word frequency. Additionally, we overlaid a horizontal line of the mean restaurant rating as a metric to determine the accuracy of our classifier. We would expect all words above to be blue and all words below to be red.

```{r afinn, echo = F, message = F, fig.align = "center", fig.height = 6}
summaries <- read_csv("../clean_data/summaries.csv")
summaries %>% 
  ggplot(aes(reviews, average_star, color = score)) +
  geom_point() +
  geom_text(aes(label = word, family = "Times New Roman"), 
            check_overlap = TRUE, vjust = 1, hjust = 1) +
  scale_x_log10(limits = c(200, 100000), 
                breaks = c(1000, 10000), labels = c(1000, 10000)) +
  geom_hline(yintercept = mean(reviews$stars), color = "black", lty = 2) +
  scale_color_gradient2(low = "red", high = "blue", 
                        midpoint = 0, mid = "grey") +
  labs(x = "# of reviews",
       y = "Average Yelp Stars",
       color = "Valence Score") +
  scale_y_continuous(limits = c(1, 5), breaks = seq(1, 5, 1)) +
  theme(text = element_text(family = "Times New Roman"))

positive <- summaries %>% 
  filter(score > 0)
negative <- summaries %>% 
  filter(score < 0)
percent_pos <- PercentAccuracy(positive)
percent_neg <- 1 - PercentAccuracy(negative)
print(str_c("Positivity Accuracy: ", percent_pos))
print(str_c("Negativity Accuracy: ", percent_neg))
```

For the most part, our classifier was accurate. 65.02% of positive words sampled lied above the mean restaurant rating, and even better, 86.99% of negative words lied below the mean star rating. Therefore, it seems reasonable to infer that there is indeed a positive correlation between valence score and Yelp business rating.

However, we also wanted to see these words plotted by their valence score versus average star rating to visualize the actual trend between sentiment and rating, rather than just viewing what percentage of words lied above the mean rating.

```{r trend, echo = FALSE, message = F, fig.align = "center"}
summaries %>% 
  arrange(desc(reviews)) %>% 
  ggplot(aes(score, average_star)) +
  geom_point(aes(size = reviews)) +
  geom_text(aes(label = word, family = "Times New Roman"),
            vjust = 1, hjust = 1, check_overlap = TRUE) +
  geom_smooth(method = "lm", se = FALSE) +
  xlab("Valence Score") +
  ylab("Average Yelp Stars") +
  labs(title = "Average Yelp Star Rating vs. Valence Score") +
  scale_x_continuous(limits = c(-5,6), breaks = seq(-5, 5, 2)) +
  scale_y_continuous(limits = c(1, 5), breaks = seq(1, 5, 1)) +
  expand_limits(x = -6) +
  theme(text = element_text(family = "Times New Roman"))
```

Based on a simple linear regression line, words with higher scores are clustered towards 4 and 5-star ratings, while most negative words have pretty low average ratings. Some words like "damn" had higher star ratings despite their low AFINN score, but that could be attributed to writers' different expressions like "damn good" for really good food. 

Lastly, we wanted to see what the mean rating was for each AFINN score since that would clearly indicate a correlation. We expected that lower scores would have lower mean business ratings, and higher scores would have higher mean ratings.

```{r bar, echo = F, message = F, fig.align = "center"}
data.frame(
  x = factor(-5:5), 
  y = unlist(-5:5 %>% map(MeanStar))) %>% 
  filter(!is.nan(y)) %>% 
  ggplot(aes(x = x, y = y, fill = x)) +
  geom_bar(stat = "identity") +
  labs(x = "Valence Score",
       y = "Average Star Rating",
       fill = "Valence Score") +
  theme(text = element_text(family = "Times New Roman"))
```

As expected, higher AFINN scores did in fact result in higher mean business ratings. Interestingly, words with a -5 AFINN rating had a higher rating than words with a -4 and -3 AFINN rating. We're not exactly sure what would've caused this, but it seems like an interesting thing to explore for further research. 