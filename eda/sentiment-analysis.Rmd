---
title: "sentiment-analysis"
author: "Anuj Desai"
date: "August 1, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(jsonlite)
library(stringr)
library(ggplot2)
library(dplyr)
library(tidytext)
library(tm)
```

## Sentiment Analysis on Text Reviews for Yelp

Sentiment analysis, or opinion mining, makes use of natural language processing to analyze pieces of text and determine the attitude of the speaker or writer. In order to find relevant lexicons to explore this topic with the Yelp review data, I researched different lexicons and found ANEW (Affective Norms for English Words), a "sentiment lexcion" that scores words for valence. 

However, upon further research, I came across an article written by Finn ËšArup Nielsen, where he compares a sentiment lexicon he created to ANEW. His list, AFINN, contains a list of English words and assigns an integer from -5 (negative) to +5 (positive) to each word based on its sentiment. He concluded that his word list better classifies sentiment rating than ANEW on Twitter sentiment analysis. Since Yelp ratings are similar to tweets, I'll use AFINN for this exploration. 

To see the full study, check out the paper found here: http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/6006/pdf/imm6006.pdf.

I'll read in the first 500,000 observations of the Yelp reviewers data, which has been written to a csv file.
```{r}
reviews <- read_csv("../clean_data/reviews.csv")
```

Now to apply this sentiment analysis to the Yelp review dataset, I have to mutate the data so that each word for every review exists in one row. I also need to remove stop words like "the", "and", "I", etc. since they don't indicate anything about the attitude of the reviewer and shouldn't affect the valence score. A library called __tm__ for text mining can eliminate stop words.

```{r, eval = F}
review_words <- reviews %>% 
  mutate(words = str_split(text, "[ ,.]")) %>% 
  unnest(words)

# create corpus for tm
corpus <- Corpus(VectorSource(review_words$words))
# Convert to lower-case
corpus <- tm_map(corpus, tolower)
# remove stop_words
corpus <- tm_map(corpus, function(x) removeWords(x,stopwords()))

head(review_words, n = 5)
```

Too slow. I'll try the library __tidytext__. This library also let's me generate valence scores using the AFINN library without having to write an unnecessarily long function. 

```{r, eval = T}
# unnest_tokens splits column into tokens using tokenizers package
# splits "text" column into individual words to analyze sentiment for further
# exploration
reviews_text <- reviews %>% 
  select(business_id, review_id, stars, text) %>%
  unnest_tokens(word, text) %>% 
  anti_join(stop_words) %>% 
  arrange(review_id)

# rank words from -5 (negativity) to 5 (positivity) using AFINN lexicon
# sentiments: 
#   data frame with 4 variables: word, sentiment, lexicon, score
# lexicon can be "nrc", "bing", or "AFINN"
valence_score <- sentiments %>% 
  filter(lexicon == "AFINN") %>% 
  select(word, score)

# calculate mean for each group of stars
# Tidied csv: review_id, star rating, average sentiment score
review_scores <- reviews_text %>% 
  inner_join(valence_score, by = "word") %>% 
  group_by(review_id, stars) %>% 
  summarise(sentiment = mean(score))
```

Pretty fast. I'll stick to this library and generate five box-plots to see if there's a positive correlation between mean sentiment score and restaurant rating.

```{r, eval = T}
# boxplot for each group
ggplot(review_scores) +
  geom_boxplot(aes(stars, sentiment, group = stars)) +
  labs(x = "Business Star Rating",
       y = "Mean Sentiment Score",
       title = "Mean Sentiment Score vs. Business Star Rating") +
  theme_bw()
```

Positive correlation exists! Some inaccuracy (outliers on boxplots). Find out most common positive and negative words in reviews, creating a summary of the counts.
```{r}
# create summary of words and their average sentiment score in each unique
# review and filter for >= 500 reviews and >= 10 businesses
# Tidied csv: word, # of businesses word appears, # of reviews word appears,
# # of different uses of word, average stars of word
review_words <- reviews_text %>% 
  count(review_id, business_id, stars, word) %>% 
  ungroup()
summaries <- review_words %>% 
  group_by(word) %>% 
  summarise(business = n_distinct(business_id),
            reviews = n(),
            use = sum(n),
            average_star = mean(stars)) %>% 
  ungroup() %>% 
  filter(reviews >= 500, business >= 10) %>% 
  inner_join(valence_score, by = "word")
head(summaries, n = 10)
```


```{r fig.height = 7, fig.width = 7}
summaries %>% 
  ggplot(aes(reviews, average_star, color = score)) +
  geom_point() +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1, hjust = 1) +
  scale_x_log10(limits = c(200, 100000), 
                breaks = c(1000, 10000), labels = c(1000, 10000)) +
  geom_hline(yintercept = mean(reviews$stars), color = "black", lty = 2) +
  scale_color_gradient2(low = "red", high = "blue", midpoint = 0, mid = "gray") +
  labs(x = "# of reviews",
       y = "Average Stars",
       color = "AFINN") +
  theme_bw()
```
