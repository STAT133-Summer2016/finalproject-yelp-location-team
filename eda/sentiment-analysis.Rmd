---
title: "sentiment-analysis"
author: "Anuj Desai"
date: "August 1, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(jsonlite)
library(stringr)
library(ggplot2)
library(dplyr)
library(tidytext)
library(tm)
library(ggrepel)
library(purrr)
source("../functions/PercentAccuracy.R")
source("../functions/MeanStar.R")
```

## Sentiment Analysis on Text Reviews for Yelp

Sentiment analysis, or opinion mining, makes use of natural language processing to analyze pieces of text and determine the attitude of the speaker or writer. I researched different lexicons and found ANEW (Affective Norms for English Words), a "sentiment lexcion" that scores words for valence. 

However, upon further research, I came across an article written by Finn ËšArup Nielsen, where he compares a sentiment lexicon he created to ANEW. His list, AFINN, contains a list of English words and assigns an integer from -5 (negative) to +5 (positive) to each word based on its sentiment. He concluded that his word list better classifies sentiment rating than ANEW on Twitter sentiment analysis. Since Yelp ratings are similar to tweets, I'll use AFINN for this exploration. 

To see the full study, check out the paper found here: http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/6006/pdf/imm6006.pdf.

I'll read 500,000 randomly sampled observations of the Yelp reviewers data, which has been written to a csv file.
```{r}
reviews <- read_csv("../clean_data/yelp_reviews.csv")
reviews <- reviews %>% 
  sample_n(500000)
```

Now to apply this sentiment analysis to the Yelp review dataset, I have to mutate the data so that each word for every review exists in one row. I also need to remove stop words like "the", "and", "I", etc. since they don't indicate anything about the attitude of the reviewer and shouldn't affect the valence score. A library called __tm__ for text mining can eliminate stop words.

```{r, eval = F}
review_words <- reviews %>% 
  mutate(words = str_split(text, "[ ,.]")) %>% 
  unnest(words)

# create corpus for tm
corpus <- Corpus(VectorSource(review_words$words))
# Convert to lower-case
corpus <- tm_map(corpus, tolower)
# remove stop_words
corpus <- tm_map(corpus, function(x) removeWords(x,stopwords()))

head(review_words, n = 5)
```

Too slow. I'll try the library __tidytext__. This library also let's me generate valence scores using the AFINN library without mhaving to write an unnecessarily long function. 

```{r, eval = T}
# unnest_tokens splits column into tokens using tokenizers package
# splits "text" column into individual words to nalyze sentiment for further
# exploration
reviews_text <- reviews %>% 
  select(business_id, review_id, stars, text) %>%
  unnest_tokens(word, text) %>% 
  anti_join(stop_words) %>% 
  arrange(review_id)

write_csv(reviews_text, "../clean_data/reviews_text.csv")

# rank words from -5 (negativity) to 5 (positivity) using AFINN lexicon
# sentiments: 
#   data frame with 4 variables: word, sentiment, lexicon, score
# lexicon can be "nrc", "bing", or "AFINN"
valence_score <- sentiments %>% 
  filter(lexicon == "AFINN") %>% 
  select(word, score)

# calculate mean for each review
# Tidied: review_id, star rating, average sentiment score
review_scores <- reviews_text %>% 
  inner_join(valence_score, by = "word") %>% 
  group_by(review_id, stars) %>% 
  summarise(sentiment = mean(score))
write_csv(review_scores, "../clean_data/review_scores.csv")
```

Pretty fast. I'll stick to this library and generate five box-plots to see if there's a positive correlation between mean sentiment score and restaurant rating.

```{r, eval = T}
# boxplot for each group
# ggplot(review_scores, aes(star, sentiment)) +
#   geom_bar(stat = "identity")
# coloring too messy, histogram/bar graph doesn't capture whole picture
# want to see if some 1-star ratings have 5s and vice versa
# boxplot shows this with outliers
ggplot(review_scores) +
  geom_boxplot(aes(stars, sentiment, group = stars)) +
  labs(x = "Business Star Rating",
       y = "Mean Sentiment Score",
       title = "Mean Sentiment Score vs. Business Star Rating")
```

Positive correlation exists! Some inaccuracy (outliers on boxplots). Find out most common positive and negative words in reviews, creating a summary of the counts. I filtered for greater than 500 reviews and 10 business so they're not unique words to a local business or rare words with strange sentiments.
```{r}
# create summary of words and their average sentiment score in each unique
# review and filter for >= 500 reviews and >= 10 businesses
# Tidied: word, # of businesses word appears, # of reviews word appears,
# average stars of word
review_words <- reviews_text %>% 
  count(review_id, business_id, stars, word) %>% 
  ungroup()
summaries <- review_words %>% 
  group_by(word) %>% 
  summarise(business = n_distinct(business_id),
            reviews = n(),
            average_star = mean(stars)) %>% 
  ungroup() %>% 
  filter(reviews >= 500, business >= 10) %>% 
  inner_join(valence_score, by = "word")
head(summaries, n = 10)
write_csv(summaries, "../clean_data/summaries.csv")
```

Now, I'll plot word average star rating vs. word frequency to see what words scored positive ratings. I'll also color by AFINN score to visualize inaccuracies and determine how well they were classified using the mean star rating of all reviews as a baseline metric.

```{r fig.height = 7, fig.width = 7}
# x scale log - graph messy otherwise
# ggrepel - too much text, unreadable 
# line color black and dotted = most visible
# color scheme most visible
# plot less points? not meaningful enough then
summaries %>% 
  ggplot(aes(reviews, average_star, color = score)) +
  geom_point() +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1, hjust = 1) +
  scale_x_log10(limits = c(200, 100000), 
                breaks = c(1000, 10000), labels = c(1000, 10000)) +
  geom_hline(yintercept = mean(reviews$stars), color = "black", lty = 2) +
  scale_color_gradient2(low = "red", high = "blue", 
                        midpoint = 0, mid = "grey") +
  labs(x = "# of reviews",
       y = "Average Yelp Stars",
       color = "Valence Score") +
  scale_y_continuous(limits = c(1, 5), breaks = seq(1, 5, 1))
```

Blue words mostly lie above the line and red words lie below so reveiw text is a pretty good predictor of business rating. Let's see how accurate it was.

```{r}
positive <- summaries %>% 
  filter(score > 0)
negative <- summaries %>% 
  filter(score < 0)
percent_pos <- PercentAccuracy(positive)
percent_neg <- 1 - PercentAccuracy(negative)
print(str_c("Positivity Accuracy: ", percent_pos))
print(str_c("Negativity Accuracy: ", percent_neg))
```
63.71% of positive words lie above the mean average rating! There's definitely a positive correlation between word positivity in text reviews and Yelp business star ratings. Word negativity is even better at 87.76% accuracy of negative words lying below the mean average rating.

This is pretty informative, but I want to see these words plotted by their valence score and average Yelp rating to visualize if there's a positive correlation here between sentiment score and their star rating. 
```{r fig.height = 7}
# boxplot again?
# see specific words and overall trend
# line doesn't fit that well but shows positive trend so best fit
summaries %>% 
  arrange(desc(reviews)) %>% 
  ggplot(aes(score, average_star)) +
  geom_point(aes(size = reviews)) +
  geom_text(aes(label = word), vjust = 1, hjust = 1, check_overlap = TRUE) +
  geom_smooth(method = "loess", se = FALSE) +
  xlab("Valence Score") +
  ylab("Average Yelp Stars") +
  labs(title = "Average Yelp Star Rating vs. Valence Score") +
  scale_x_continuous(limits = c(-5,6), breaks = seq(-5, 5, 2)) +
  scale_y_continuous(limits = c(1, 5), breaks = seq(1, 5, 1)) +
  expand_limits(x = -6)
```

Based on a simple linear regression line, words with higher scores are clustered towards 4 and 5-star ratings, while most negative words have pretty low average ratings. Some words like "damn" had higher star ratings despite their low AFINN score, but that could be explained by reviews where writers could have described their food or service as "damn good".

```{r}
# histogram complains
# bar graph good (Y)
data.frame(
  x = factor(-5:5), 
  y = unlist(-5:5 %>% map(MeanStar))) %>% 
  filter(!is.nan(y)) %>% 
  ggplot(aes(x = x, y = y, fill = x)) +
  geom_bar(stat = "identity") +
  labs(x = "Valence Score",
       y = "Average Star Rating",
       fill = "Valence Score")
```